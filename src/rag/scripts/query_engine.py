"""
Query engine module for RAG system
Handles question answering with context retrieval
"""

from typing import Optional, Dict, List
from llama_index.core import VectorStoreIndex
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.response_synthesizers import get_response_synthesizer
from llama_index.core.prompts import PromptTemplate
from config import Config
from vector_store import VectorStoreManager
from llm_config import LLMConfigurator

class RAGQueryEngine:
    """RAG Query Engine for answering questions"""
    
    QA_PROMPT_TEMPLATE = """
B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ gi√°o d·ª•c ƒë·∫°i h·ªçc t·∫°i Vi·ªát Nam. B·∫°n PH·∫¢I tu√¢n th·ªß c√°c quy t·∫Øc sau:

QUAN TR·ªåNG - QUY T·∫ÆC B·∫ÆT BU·ªòC:
1. B·∫ÆT BU·ªòC tr·∫£ l·ªùi HO√ÄN TO√ÄN b·∫±ng ti·∫øng Vi·ªát
2. KH√îNG ƒë∆∞·ª£c tr·∫£ l·ªùi b·∫±ng ti·∫øng Anh d√π trong b·∫•t k·ª≥ tr∆∞·ªùng h·ª£p n√†o
3. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi
4. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu"

NG·ªÆ C·∫¢NH T√ÄI LI·ªÜU:
{context_str}

C√ÇU H·ªéI C·ª¶A SINH VI√äN: {query_str}

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:
- ƒê·ªçc k·ªπ ng·ªØ c·∫£nh v√† t√¨m th√¥ng tin li√™n quan tr·ª±c ti·∫øp ƒë·∫øn c√¢u h·ªèi
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng b·∫±ng ti·∫øng Vi·ªát
- Tr√≠ch d·∫´n ch√≠nh x√°c t·ª´ t√†i li·ªáu n·∫øu c√≥
- N·∫øu c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn t√†i li·ªáu, h√£y n√≥i r√µ
- S·ª≠ d·ª•ng bullet points ƒë·ªÉ li·ªát k√™ th√¥ng tin n·∫øu c·∫ßn

TR·∫¢ L·ªúI (B·∫ÆT BU·ªòC B·∫∞NG TI·∫æNG VI·ªÜT):
"""
    
    def __init__(self, index: Optional[VectorStoreIndex] = None):
        self.config = Config
        self.index = index
        self.query_engine = None
        
        # Initialize LLM
        self.llm_config = LLMConfigurator()
        
        # Setup query engine if index provided
        if self.index:
            self._setup_query_engine()
    
    def load_or_create_index(self) -> bool:
        """Load existing index or return False if not found"""
        try:
            vector_manager = VectorStoreManager()
            
            # Try to load existing index
            index = vector_manager.load_index()
            
            if index is None:
                print("\n‚ö† No existing index found!")
                print("üí° Please add documents to the data directory")
                print("üí° Then run: python scripts/vector_store.py")
                return False
            
            self.index = index
            self._setup_query_engine()
            return True
            
        except Exception as e:
            print(f"‚ùå Error loading index: {e}")
            return False
    
    def _setup_query_engine(self):
        """Setup query engine with retriever and response synthesizer"""
        print("\nüîß Setting up query engine...")
        
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=5  # Increased from 3 to 5 for better context
        )
        
        # Create custom prompt
        qa_prompt = PromptTemplate(self.QA_PROMPT_TEMPLATE)
        
        response_synthesizer = get_response_synthesizer(
            text_qa_template=qa_prompt,
            response_mode="refine"  # Better quality than 'compact'
        )
        
        # Create query engine
        self.query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=response_synthesizer
        )
        
        print("‚úì Query engine configured successfully")
        print("  - Retrieval: Top 5 most relevant chunks")
        print("  - Response mode: Refine (high quality)")
    
    def query(self, question: str) -> Dict:
        """
        Query the RAG system with a question
        
        Args:
            question: User's question
            
        Returns:
            Dictionary with response and metadata
        """
        if not self.query_engine:
            if not self.index:
                success = self.load_or_create_index()
                if not success:
                    raise RuntimeError("Cannot query: No vector index available")
            else:
                self._setup_query_engine()
        
        print(f"\n‚ùì Question: {question}")
        
        try:
            print("‚è≥ ƒêang x·ª≠ l√Ω c√¢u h·ªèi... (c√≥ th·ªÉ m·∫•t 30-60 gi√¢y)")
            
            # Execute query with timeout handling
            response = self.query_engine.query(question)
            
            # Extract source information
            sources = []
            if hasattr(response, 'source_nodes'):
                for node in response.source_nodes:
                    sources.append({
                        'text': node.text[:200] + "...",
                        'score': node.score,
                        'metadata': node.metadata
                    })
            
            result = {
                'answer': str(response),
                'sources': sources,
                'question': question
            }
            
            print(f"\nüí° Answer: {result['answer'][:200]}...")
            
            return result
            
        except TimeoutError as e:
            print("\n‚ùå Timeout Error: LLM kh√¥ng ph·∫£n h·ªìi trong th·ªùi gian cho ph√©p")
            print("\nüîß C√°c b∆∞·ªõc kh·∫Øc ph·ª•c:")
            print("1. Ki·ªÉm tra Ollama ƒëang ch·∫°y: ollama list")
            print("2. Kh·ªüi ƒë·ªông Ollama n·∫øu ch∆∞a ch·∫°y: ollama serve")
            print("3. Ki·ªÉm tra model ƒë√£ t·∫£i: ollama list")
            print(f"4. T·∫£i model n·∫øu ch∆∞a c√≥: ollama pull {self.config.OLLAMA_MODEL}")
            print("5. Th·ª≠ model nh·∫π h∆°n: ollama pull llama3.2:1b")
            raise
        except Exception as e:
            print(f"\n‚ùå Error during query: {e}")
            print("\nüîß Ki·ªÉm tra:")
            print("1. Ollama ƒëang ch·∫°y: ollama serve")
            print(f"2. Model ƒë√£ t·∫£i: ollama pull {self.config.OLLAMA_MODEL}")
            print("3. K·∫øt n·ªëi m·∫°ng ·ªïn ƒë·ªãnh")
            raise

    def chat(self):
        """Interactive chat mode"""
        print("\n" + "=" * 60)
        print("ü§ñ RAG Educational Chatbot")
        print("=" * 60)
        print("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (ho·∫∑c 'exit' ƒë·ªÉ tho√°t)")
        print("=" * 60 + "\n")
        
        # Ensure index is loaded
        if not self.index:
            success = self.load_or_create_index()
            if not success:
                print("\n‚ùå Cannot start chat: No documents indexed")
                print("üí° Please add documents and build the index first")
                return
        
        while True:
            try:
                question = input("\nüë§ B·∫°n: ").strip()
                
                if not question:
                    continue
                
                if question.lower() in ['exit', 'quit', 'tho√°t']:
                    print("\nüëã T·∫°m bi·ªát!")
                    break
                
                # Get response
                result = self.query(question)
                
                print(f"\nü§ñ Bot: {result['answer']}")
                
                # Show sources if available
                if result['sources']:
                    print(f"\nüìö Ngu·ªìn tham kh·∫£o ({len(result['sources'])} t√†i li·ªáu):")
                    for i, source in enumerate(result['sources'][:3], 1):
                        print(f"\n  {i}. {source['metadata'].get('filename', 'Unknown')}")
                        print(f"     ƒê·ªô li√™n quan: {source['score']:.2f}")
                        print(f"     Tr√≠ch d·∫´n: {source['text'][:150]}...")
                
            except KeyboardInterrupt:
                print("\n\nüëã T·∫°m bi·ªát!")
                break
            except Exception as e:
                print(f"\n‚ùå L·ªói: {e}")

def main():
    """Main function to run query engine"""
    try:
        # Initialize query engine
        engine = RAGQueryEngine()
        
        # Start interactive chat
        engine.chat()
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
